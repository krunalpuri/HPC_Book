\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{demchenko2014defining}
\select@language{american}
\@writefile{toc}{\select@language{american}}
\@writefile{lof}{\select@language{american}}
\@writefile{lot}{\select@language{american}}
\@writefile{toc}{\contentsline {title}{Big Data with High Performance Computing (HPC)}{1}{chapter.1}}
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Tarek El-Ghazawi \and Krunal Puri \and Armin Mehrabian}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{1}}
\citation{shvachko2010hadoop}
\citation{ghemawat2003google}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Matrix of data and computation applications.}}{2}{figure.1.1}}
\newlabel{fig:bigdata_hpc_matrix}{{1}{2}{Matrix of data and computation applications}{figure.1.1}{}}
\newlabel{fig:bigdata_hpc_matrix@cref}{{[figure][1][]1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Big Data Ecosystem}{2}{subsection.1.1.1}}
\citation{vavilapalli2013apache}
\citation{borthakur2011apache}
\citation{zaharia2012resilient}
\citation{braam2004lustre}
\citation{FrankSchmuck}
\citation{shaun2007storage}
\citation{rajasekar2010irods}
\citation{grimshaw2013gffs}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A general big data ecosystem architecture.}}{3}{figure.1.2}}
\newlabel{fig:bigdata_ecosystem}{{2}{3}{A general big data ecosystem architecture}{figure.1.2}{}}
\newlabel{fig:bigdata_ecosystem@cref}{{[figure][2][]2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}HPC Ecosystem}{3}{subsection.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Apache Big Data Stack (ABDS) ecosystem.}}{4}{figure.1.3}}
\newlabel{fig:ABDS_ecosystem}{{3}{4}{Apache Big Data Stack (ABDS) ecosystem}{figure.1.3}{}}
\newlabel{fig:ABDS_ecosystem@cref}{{[figure][3][]3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of HPC ecosystem.}}{4}{figure.1.4}}
\newlabel{fig:HPC_ecosystem}{{4}{4}{An example of HPC ecosystem}{figure.1.4}{}}
\newlabel{fig:HPC_ecosystem@cref}{{[figure][4][]4}{4}}
\citation{yoo2003slurm}
\citation{staples2006torque}
\citation{gentzsch2001sun}
\citation{plimpton2011mapreduce}
\citation{mantha2012pilot}
\citation{ekanayake2010twister}
\citation{luckow2012p}
\citation{raicu2008many}
\citation{deelman2009workflows}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Big Data Applications}{5}{subsection.1.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Integration of big data and HPC ecosystems}{6}{subsection.1.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}State of the art projects}{6}{section.1.2}}
\citation{qiu2014towards}
\citation{qiu2014towards}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Integration of HPC and big data stack}{7}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{High Performance Big Data System (HPBDS) project \cite  {qiu2014towards}}{7}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MIDAS and SPIDAL within HPC-ABDS stack\cite  {qiu2014towards}}}{7}{figure.1.5}}
\newlabel{fig:MIDAS_SPIDAL}{{5}{7}{MIDAS and SPIDAL within HPC-ABDS stack\cite {qiu2014towards}}{figure.1.5}{}}
\newlabel{fig:MIDAS_SPIDAL@cref}{{[figure][5][]5}{7}}
\citation{fox2014towards}
\citation{avery2011giraph}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Five programming models identified for big data HPC problems based on NIST big data use cases.}}{8}{table.1.1}}
\newlabel{table:NIST_5_Programming_model}{{1}{8}{Five programming models identified for big data HPC problems based on NIST big data use cases}{table.1.1}{}}
\newlabel{table:NIST_5_Programming_model@cref}{{[table][1][]1}{8}}
\citation{aji2013hadoop}
\citation{domo}
\citation{hinton2006fast}
\citation{domo}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and HPC}{10}{subsection.1.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces One minute of Internet usage \cite  {domo}.}}{11}{table.1.2}}
\newlabel{table:Domo}{{2}{11}{One minute of Internet usage \cite {domo}}{table.1.2}{}}
\newlabel{table:Domo@cref}{{[table][2][]2}{11}}
\citation{chilimbi2014project}
\citation{le2013building}
\citation{deng2009imagenet}
\citation{farber1997parallel}
\citation{chilimbi2014project}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Data parallelism performing Gradient Descent (GD). Model replicas at each node calculate gradients $\Delta w $ on a piece of data. Gradients are later transmitted to global model server to calculate updated weights.}}{12}{figure.1.6}}
\newlabel{fig:data_parallelism}{{6}{12}{Data parallelism performing Gradient Descent (GD). Model replicas at each node calculate gradients $\Delta w $ on a piece of data. Gradients are later transmitted to global model server to calculate updated weights}{figure.1.6}{}}
\newlabel{fig:data_parallelism@cref}{{[figure][6][]6}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An example of model parallelism. Each node store a fragment of the overall model and a whole copy of data.}}{13}{figure.1.7}}
\newlabel{fig:model_parallelism}{{7}{13}{An example of model parallelism. Each node store a fragment of the overall model and a whole copy of data}{figure.1.7}{}}
\newlabel{fig:model_parallelism@cref}{{[figure][7][]7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model and data partitioning in Adam architecture\cite  {chilimbi2014project}.}}{14}{figure.1.8}}
\newlabel{fig:adam_parallelism}{{8}{14}{Model and data partitioning in Adam architecture\cite {chilimbi2014project}}{figure.1.8}{}}
\newlabel{fig:adam_parallelism@cref}{{[figure][8][]8}{14}}
\citation{simard2003best}
\citation{goodfellow2013maxout}
\citation{le2013building}
\citation{le2013building}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Adam accuracy on MNIST benchmark compared to the existing top accuracy model.}}{16}{table.1.3}}
\newlabel{table:MNIST}{{3}{16}{Adam accuracy on MNIST benchmark compared to the existing top accuracy model}{table.1.3}{}}
\newlabel{table:MNIST@cref}{{[table][3][]3}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Adam accuracy on ImageNet benchmark compared to the existing top accuracy model.}}{16}{table.1.4}}
\newlabel{table:ImageNet}{{4}{16}{Adam accuracy on ImageNet benchmark compared to the existing top accuracy model}{table.1.4}{}}
\newlabel{table:ImageNet@cref}{{[table][4][]4}{16}}
\citation{chen2014dadiannao}
\citation{temam2012defect}
\citation{esmaeilzadeh2012neural}
\citation{chen2014diannao}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Block diagram of the DianNao accelerator\cite  {chen2014dadiannao}.}}{17}{figure.1.9}}
\newlabel{fig:diannao_arch}{{9}{17}{Block diagram of the DianNao accelerator\cite {chen2014dadiannao}}{figure.1.9}{}}
\newlabel{fig:diannao_arch@cref}{{[figure][9][]9}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Speedup of CPU/GPU and CPU/DianNao for various layer types of network\cite  {chen2014dadiannao}.}}{18}{figure.1.10}}
\newlabel{fig:diannao_speedup}{{10}{18}{Speedup of CPU/GPU and CPU/DianNao for various layer types of network\cite {chen2014dadiannao}}{figure.1.10}{}}
\newlabel{fig:diannao_speedup@cref}{{[figure][10][]10}{18}}
\citation{chen2014diannao}
\citation{chen2014dadiannao}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Block diagram of a node within Dadiannao architecture \cite  {chen2014dadiannao}.}}{19}{figure.1.11}}
\newlabel{fig:NFU_Diagram}{{11}{19}{Block diagram of a node within Dadiannao architecture \cite {chen2014dadiannao}}{figure.1.11}{}}
\newlabel{fig:NFU_Diagram@cref}{{[figure][11][]11}{19}}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Tile implementation of a node \cite  {chen2014dadiannao}.}}{20}{figure.1.12}}
\newlabel{fig:tiles}{{12}{20}{Tile implementation of a node \cite {chen2014dadiannao}}{figure.1.12}{}}
\newlabel{fig:tiles@cref}{{[figure][12][]12}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Speedup of Dadiannao/GPU for various types of layers. CONV1 and fullNN layers require at least 4 nodes. CONV3 and CONV4 need at least 36 nodes to operate\cite  {chen2014dadiannao}.}}{21}{figure.1.13}}
\newlabel{fig:dadiannao_speedup}{{13}{21}{Speedup of Dadiannao/GPU for various types of layers. CONV1 and fullNN layers require at least 4 nodes. CONV3 and CONV4 need at least 36 nodes to operate\cite {chen2014dadiannao}}{figure.1.13}{}}
\newlabel{fig:dadiannao_speedup@cref}{{[figure][13][]13}{21}}
\citation{avery2011giraph}
\citation{low2014graphlab}
\citation{seo2010hama}
\citation{malewicz2010pregel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}High Performance Graph Processing}{22}{subsection.1.2.3}}
\citation{sakr2013processing}
\citation{sakr2013processing}
\citation{dewitt1983database}
\citation{dewitt1992parallel}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Execution mechanism of BSP programming model.}}{23}{figure.1.14}}
\newlabel{fig:BSP}{{14}{23}{Execution mechanism of BSP programming model}{figure.1.14}{}}
\newlabel{fig:BSP@cref}{{[figure][14][]14}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Parallel Databases and I/O}{23}{subsection.1.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Example of computing maximum vertex value. At each superstep values for each vertex are sent to the neighbor. If the received value is higher than the current value, current value is updated. Otherwise, current value is hold \cite  {sakr2013processing}.}}{24}{figure.1.15}}
\newlabel{fig:max_super}{{15}{24}{Example of computing maximum vertex value. At each superstep values for each vertex are sent to the neighbor. If the received value is higher than the current value, current value is updated. Otherwise, current value is hold \cite {sakr2013processing}}{figure.1.15}{}}
\newlabel{fig:max_super@cref}{{[figure][15][]15}{24}}
\citation{zhou2012scope}
\citation{zhou2012scope}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Architecture of COSMOS with three main components\cite  {zhou2012scope}.}}{26}{figure.1.16}}
\newlabel{fig:krunal8}{{16}{26}{Architecture of COSMOS with three main components\cite {zhou2012scope}}{figure.1.16}{}}
\newlabel{fig:krunal8@cref}{{[figure][16][]16}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Compilation of a SCOPE script.}}{27}{figure.1.17}}
\newlabel{fig:compilation_of_a_scope}{{17}{27}{Compilation of a SCOPE script}{figure.1.17}{}}
\newlabel{fig:compilation_of_a_scope@cref}{{[figure][17][]17}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Code Generation and Runtime Engine: }{27}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Execution Model: }{28}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Job Scheduling: }{28}{section*.18}}
\citation{riha2013adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces SCOPE job scheduling }}{29}{figure.1.18}}
\newlabel{fig:scope_job_sched}{{18}{29}{SCOPE job scheduling}{figure.1.18}{}}
\newlabel{fig:scope_job_sched@cref}{{[figure][18][]18}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The NUMA (Non-Uniform Memory Access) effect in two CPU with private memory and memory controller. The system has a shared memory address space but memory access has non uniform latency. Each processor has quicker access to its own physical memory and some delay while accessing other CPU's physical memory }}{30}{figure.1.19}}
\newlabel{fig:nume_something}{{19}{30}{The NUMA (Non-Uniform Memory Access) effect in two CPU with private memory and memory controller. The system has a shared memory address space but memory access has non uniform latency. Each processor has quicker access to its own physical memory and some delay while accessing other CPU's physical memory}{figure.1.19}{}}
\newlabel{fig:nume_something@cref}{{[figure][19][]19}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Sub-Cube is partitioned amongst two physical memory. Scenario 1 shows the most ideal case where partitions are equally distributed. Scenario 2 show a suboptimal case where either of the CPU have only 25\% of the data. And 3rd scenario shows worst case when a whole sub-cube is inside only single CPU's memory. }}{30}{figure.1.20}}
\newlabel{fig:Cuboid_divide}{{20}{30}{Sub-Cube is partitioned amongst two physical memory. Scenario 1 shows the most ideal case where partitions are equally distributed. Scenario 2 show a suboptimal case where either of the CPU have only 25\% of the data. And 3rd scenario shows worst case when a whole sub-cube is inside only single CPU's memory}{figure.1.20}{}}
\newlabel{fig:Cuboid_divide@cref}{{[figure][20][]20}{30}}
\citation{riha2013adaptive}
\citation{riha2013adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Block diagram of system partition. The scheduler assigns MOLAP cube processing task to CPU and all the queries on fact table are directed to GPU partitions. Each GPU partition contains different number of streaming processor(SM) \cite  {riha2013adaptive}}}{31}{figure.1.21}}
\newlabel{fig:krunal6}{{21}{31}{Block diagram of system partition. The scheduler assigns MOLAP cube processing task to CPU and all the queries on fact table are directed to GPU partitions. Each GPU partition contains different number of streaming processor(SM) \cite {riha2013adaptive}}{figure.1.21}{}}
\newlabel{fig:krunal6@cref}{{[figure][21][]21}{31}}
\citation{zou2014improving}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Nodes are arranged into three level in simulation analysis. Compute nodes can carry out simulation and some nodes act as a helper nodes performing intermediate analytics. Staging nodes only performs analytics and their output is passed on to I/O nodes, responsible for storage. Label 1, 2 and 3 point out the tentative adaptive compression location in simulation analysis.}}{32}{figure.1.22}}
\newlabel{fig:krunal7}{{22}{32}{Nodes are arranged into three level in simulation analysis. Compute nodes can carry out simulation and some nodes act as a helper nodes performing intermediate analytics. Staging nodes only performs analytics and their output is passed on to I/O nodes, responsible for storage. Label 1, 2 and 3 point out the tentative adaptive compression location in simulation analysis}{figure.1.22}{}}
\newlabel{fig:krunal7@cref}{{[figure][22][]22}{32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Tentative Selection}}{32}{algorithm.1}}
\newlabel{tentative}{{1}{32}{Improving IO performance with Adaptive Compression}{algorithm.1}{}}
\newlabel{tentative@cref}{{[algorithm][1][]1}{32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Predictive Decision}}{33}{algorithm.2}}
\newlabel{predictive}{{2}{33}{Improving IO performance with Adaptive Compression}{algorithm.2}{}}
\newlabel{predictive@cref}{{[algorithm][2][]2}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Data compression and transfer analysis of two implementation and adaptive hybrid implementation.}}{33}{figure.1.23}}
\newlabel{fig:krunal9}{{23}{33}{Data compression and transfer analysis of two implementation and adaptive hybrid implementation}{figure.1.23}{}}
\newlabel{fig:krunal9@cref}{{[figure][23][]23}{33}}
\citation{wasi2013high}
\citation{wasi2013high}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}HPC implementation of MapReduce}{34}{subsection.1.2.5}}
\citation{teijeiro2011design}
\citation{el2006upc}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces RDMA based architecture of MapReduce proposed by \cite  {wasi2013high}.}}{35}{figure.1.24}}
\newlabel{fig:RDMA_hadoop}{{24}{35}{RDMA based architecture of MapReduce proposed by \cite {wasi2013high}}{figure.1.24}{}}
\newlabel{fig:RDMA_hadoop@cref}{{[figure][24][]24}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Future Direction}{36}{section.1.3}}
\bibstyle{splncs03}
\bibdata{paper_2.bib}
\bibcite{domo}{1}
\bibcite{}{2}
\bibcite{aji2013hadoop}{3}
\bibcite{avery2011giraph}{4}
\bibcite{borthakur2011apache}{5}
\bibcite{braam2004lustre}{6}
\bibcite{chen2014big}{7}
\bibcite{chen2014diannao}{8}
\bibcite{chen2014dadiannao}{9}
\bibcite{chilimbi2014project}{10}
\bibcite{dean2012large}{11}
\bibcite{deelman2009workflows}{12}
\bibcite{demchenko2014defining}{13}
\bibcite{deng2009imagenet}{14}
\bibcite{dewitt1992parallel}{15}
\bibcite{dewitt1983database}{16}
\bibcite{ekanayake2010twister}{17}
\bibcite{el2006upc}{18}
\bibcite{esmaeilzadeh2012neural}{19}
\bibcite{farber1997parallel}{20}
\bibcite{fox2014towards}{21}
\bibcite{FrankSchmuck}{22}
\bibcite{gentzsch2001sun}{23}
\bibcite{ghemawat2003google}{24}
\bibcite{goodfellow2013maxout}{25}
\bibcite{grimshaw2013gffs}{26}
\bibcite{hinton2006fast}{27}
\bibcite{Jha2014}{28}
\bibcite{kashyap2015big}{29}
\bibcite{le2013building}{30}
\bibcite{low2014graphlab}{31}
\bibcite{luckow2012p}{32}
\bibcite{malewicz2010pregel}{33}
\bibcite{mantha2012pilot}{34}
\bibcite{plimpton2011mapreduce}{35}
\bibcite{Qiu2014}{36}
\bibcite{qiu2014towards}{37}
\bibcite{wasi2013high}{38}
\bibcite{raicu2008many}{39}
\bibcite{rajasekar2010irods}{40}
\bibcite{Reed2015}{41}
\bibcite{riha2013adaptive}{42}
\bibcite{sakr2013processing}{43}
\bibcite{seo2010hama}{44}
\bibcite{shaun2007storage}{45}
\bibcite{shvachko2010hadoop}{46}
\bibcite{simard2003best}{47}
\bibcite{staples2006torque}{48}
\bibcite{teijeiro2011design}{49}
\bibcite{temam2012defect}{50}
\bibcite{vavilapalli2013apache}{51}
\bibcite{yoo2003slurm}{52}
\bibcite{zaharia2012resilient}{53}
\bibcite{zhou2012scope}{54}
\bibcite{zou2014improving}{55}
\citation{*}
