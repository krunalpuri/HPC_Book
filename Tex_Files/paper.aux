\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{demchenko2014defining}
\select@language{american}
\@writefile{toc}{\select@language{american}}
\@writefile{lof}{\select@language{american}}
\@writefile{lot}{\select@language{american}}
\@writefile{toc}{\contentsline {title}{Big Data with High Performance Computing (HPC)}{1}{chapter.1}}
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Tarek El-Ghazawi \and Krunal Puri \and Armin Mehrabian}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{1}}
\citation{shvachko2010hadoop}
\citation{ghemawat2003google}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Matrix of data and computation applications.}}{2}{figure.1.1}}
\newlabel{fig:bigdata_hpc_matrix}{{1}{2}{Matrix of data and computation applications}{figure.1.1}{}}
\newlabel{fig:bigdata_hpc_matrix@cref}{{[figure][1][]1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Big Data Ecosystem}{2}{subsection.1.1.1}}
\citation{vavilapalli2013apache}
\citation{borthakur2011apache}
\citation{zaharia2012resilient}
\citation{braam2004lustre}
\citation{FrankSchmuck}
\citation{shaun2007storage}
\citation{rajasekar2010irods}
\citation{grimshaw2013gffs}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A general big data ecosystem architecture.}}{3}{figure.1.2}}
\newlabel{fig:bigdata_ecosystem}{{2}{3}{A general big data ecosystem architecture}{figure.1.2}{}}
\newlabel{fig:bigdata_ecosystem@cref}{{[figure][2][]2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}HPC Ecosystem}{3}{subsection.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Apache Big Data Stack (ABDS) ecosystem.}}{4}{figure.1.3}}
\newlabel{fig:ABDS_ecosystem}{{3}{4}{Apache Big Data Stack (ABDS) ecosystem}{figure.1.3}{}}
\newlabel{fig:ABDS_ecosystem@cref}{{[figure][3][]3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of HPC ecosystem.}}{4}{figure.1.4}}
\newlabel{fig:HPC_ecosystem}{{4}{4}{An example of HPC ecosystem}{figure.1.4}{}}
\newlabel{fig:HPC_ecosystem@cref}{{[figure][4][]4}{4}}
\citation{yoo2003slurm}
\citation{staples2006torque}
\citation{gentzsch2001sun}
\citation{plimpton2011mapreduce}
\citation{mantha2012pilot}
\citation{ekanayake2010twister}
\citation{luckow2012p}
\citation{raicu2008many}
\citation{deelman2009workflows}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Big Data Applications}{5}{subsection.1.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Integration of big data and HPC ecosystems}{6}{subsection.1.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}State of the art projects}{6}{section.1.2}}
\citation{qiu2014towards}
\citation{qiu2014towards}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Integration of HPC and big data stack}{7}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{High Performance Big Data System (HPBDS) project \cite  {qiu2014towards}}{7}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MIDAS and SPIDAL within HPC-ABDS stack\cite  {qiu2014towards}}}{7}{figure.1.5}}
\newlabel{fig:MIDAS_SPIDAL}{{5}{7}{MIDAS and SPIDAL within HPC-ABDS stack\cite {qiu2014towards}}{figure.1.5}{}}
\newlabel{fig:MIDAS_SPIDAL@cref}{{[figure][5][]5}{7}}
\citation{fox2014towards}
\citation{avery2011giraph}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Five programming models identified for big data HPC problems based on NIST big data use cases.}}{8}{table.1.1}}
\newlabel{table:NIST_5_Programming_model}{{1}{8}{Five programming models identified for big data HPC problems based on NIST big data use cases}{table.1.1}{}}
\newlabel{table:NIST_5_Programming_model@cref}{{[table][1][]1}{8}}
\citation{aji2013hadoop}
\citation{domo}
\citation{hinton2006fast}
\citation{domo}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and HPC}{10}{subsection.1.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces One minute of Internet usage \cite  {domo}.}}{11}{table.1.2}}
\newlabel{table:Domo}{{2}{11}{One minute of Internet usage \cite {domo}}{table.1.2}{}}
\newlabel{table:Domo@cref}{{[table][2][]2}{11}}
\citation{chilimbi2014project}
\citation{le2013building}
\citation{deng2009imagenet}
\citation{farber1997parallel}
\citation{chilimbi2014project}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Data parallelism performing Gradient Descent (GD). Model replicas at each node calculate gradients $\Delta w $ on a piece of data. Gradients are later transmitted to global model server to calculate updated weights.}}{12}{figure.1.6}}
\newlabel{fig:data_parallelism}{{6}{12}{Data parallelism performing Gradient Descent (GD). Model replicas at each node calculate gradients $\Delta w $ on a piece of data. Gradients are later transmitted to global model server to calculate updated weights}{figure.1.6}{}}
\newlabel{fig:data_parallelism@cref}{{[figure][6][]6}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An example of model parallelism. Each node store a fragment of the overall model and a whole copy of data.}}{13}{figure.1.7}}
\newlabel{fig:model_parallelism}{{7}{13}{An example of model parallelism. Each node store a fragment of the overall model and a whole copy of data}{figure.1.7}{}}
\newlabel{fig:model_parallelism@cref}{{[figure][7][]7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model and data partitioning in Adam architecture\cite  {chilimbi2014project}.}}{14}{figure.1.8}}
\newlabel{fig:adam_parallelism}{{8}{14}{Model and data partitioning in Adam architecture\cite {chilimbi2014project}}{figure.1.8}{}}
\newlabel{fig:adam_parallelism@cref}{{[figure][8][]8}{14}}
\citation{simard2003best}
\citation{goodfellow2013maxout}
\citation{le2013building}
\citation{le2013building}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Adam accuracy on MNIST benchmark compared to the existing top accuracy model.}}{16}{table.1.3}}
\newlabel{table:MNIST}{{3}{16}{Adam accuracy on MNIST benchmark compared to the existing top accuracy model}{table.1.3}{}}
\newlabel{table:MNIST@cref}{{[table][3][]3}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Adam accuracy on ImageNet benchmark compared to the existing top accuracy model.}}{16}{table.1.4}}
\newlabel{table:ImageNet}{{4}{16}{Adam accuracy on ImageNet benchmark compared to the existing top accuracy model}{table.1.4}{}}
\newlabel{table:ImageNet@cref}{{[table][4][]4}{16}}
\citation{chen2014dadiannao}
\citation{temam2012defect}
\citation{esmaeilzadeh2012neural}
\citation{chen2014diannao}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Block diagram of the DianNao accelerator\cite  {chen2014dadiannao}.}}{17}{figure.1.9}}
\newlabel{fig:diannao_arch}{{9}{17}{Block diagram of the DianNao accelerator\cite {chen2014dadiannao}}{figure.1.9}{}}
\newlabel{fig:diannao_arch@cref}{{[figure][9][]9}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Speedup of CPU/GPU and CPU/DianNao for various layer types of network\cite  {chen2014dadiannao}.}}{18}{figure.1.10}}
\newlabel{fig:diannao_speedup}{{10}{18}{Speedup of CPU/GPU and CPU/DianNao for various layer types of network\cite {chen2014dadiannao}}{figure.1.10}{}}
\newlabel{fig:diannao_speedup@cref}{{[figure][10][]10}{18}}
\citation{chen2014diannao}
\citation{chen2014dadiannao}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Block diagram of a node within Dadiannao architecture \cite  {chen2014dadiannao}.}}{19}{figure.1.11}}
\newlabel{fig:NFU_Diagram}{{11}{19}{Block diagram of a node within Dadiannao architecture \cite {chen2014dadiannao}}{figure.1.11}{}}
\newlabel{fig:NFU_Diagram@cref}{{[figure][11][]11}{19}}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\citation{chen2014dadiannao}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Tile implementation of a node \cite  {chen2014dadiannao}.}}{20}{figure.1.12}}
\newlabel{fig:tiles}{{12}{20}{Tile implementation of a node \cite {chen2014dadiannao}}{figure.1.12}{}}
\newlabel{fig:tiles@cref}{{[figure][12][]12}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Speedup of Dadiannao/GPU for various types of layers. CONV1 and fullNN layers require at least 4 nodes. CONV3 and CONV4 need at least 36 nodes to operate\cite  {chen2014dadiannao}.}}{21}{figure.1.13}}
\newlabel{fig:dadiannao_speedup}{{13}{21}{Speedup of Dadiannao/GPU for various types of layers. CONV1 and fullNN layers require at least 4 nodes. CONV3 and CONV4 need at least 36 nodes to operate\cite {chen2014dadiannao}}{figure.1.13}{}}
\newlabel{fig:dadiannao_speedup@cref}{{[figure][13][]13}{21}}
\citation{avery2011giraph}
\citation{low2014graphlab}
\citation{seo2010hama}
\citation{malewicz2010pregel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}High Performance Graph Processing}{22}{subsection.1.2.3}}
\citation{sakr2013processing}
\citation{sakr2013processing}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Execution mechanism of BSP programming model.}}{23}{figure.1.14}}
\newlabel{fig:BSP}{{14}{23}{Execution mechanism of BSP programming model}{figure.1.14}{}}
\newlabel{fig:BSP@cref}{{[figure][14][]14}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Parallel Databases and I/O}{23}{subsection.1.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Example of computing maximum vertex value. At each superstep values for each vertex are sent to the neighbor. If the received value is higher than the current value, current value is updated. Otherwise, current value is hold\cite  {sakr2013processing}.}}{24}{figure.1.15}}
\newlabel{fig:max_super}{{15}{24}{Example of computing maximum vertex value. At each superstep values for each vertex are sent to the neighbor. If the received value is higher than the current value, current value is updated. Otherwise, current value is hold\cite {sakr2013processing}}{figure.1.15}{}}
\newlabel{fig:max_super@cref}{{[figure][15][]15}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Project SCOPE: Parallel databases meet Map/Reduce}{25}{section*.13}}
\@writefile{toc}{\contentsline {subsubsection}{Platform Architecture:}{25}{section*.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Architecture of cosmos platform }}{26}{figure.1.16}}
\newlabel{fig:architecture_of_cosmos}{{16}{26}{Architecture of cosmos platform}{figure.1.16}{}}
\newlabel{fig:architecture_of_cosmos@cref}{{[figure][16][]16}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Query Language for Scope}{27}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Compilation of a SCOPE script.}}{27}{figure.1.17}}
\newlabel{fig:compilation_of_a_scope}{{17}{27}{Compilation of a SCOPE script}{figure.1.17}{}}
\newlabel{fig:compilation_of_a_scope@cref}{{[figure][17][]17}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Code Generation and Runtime Engine}{27}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Execusion Model}{28}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{Job Scheduling}{28}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{Adaptive Hybrid OLAP Architecture}{28}{section*.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces SCOPE job scheduling }}{29}{figure.1.18}}
\newlabel{fig:scope_job_sched}{{18}{29}{SCOPE job scheduling}{figure.1.18}{}}
\newlabel{fig:scope_job_sched@cref}{{[figure][18][]18}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The NUMA (Non-Uniform Memory Access) effect in shared memory system with two multicore processors. Cores of each processor equally share memory controller and memory attached to it. The access to the memory of the second processor is slower than access to its own memory }}{30}{figure.1.19}}
\newlabel{fig:nume_something}{{19}{30}{The NUMA (Non-Uniform Memory Access) effect in shared memory system with two multicore processors. Cores of each processor equally share memory controller and memory attached to it. The access to the memory of the second processor is slower than access to its own memory}{figure.1.19}{}}
\newlabel{fig:nume_something@cref}{{[figure][19][]19}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Cuboid divided into two physical memories. In ideal case (middle) half of the sub-cube is accessed through memory controller of CPU0 and the second half is accessed through memory controller of CPU1. In suboptimal case (left) 75\% of sub-cube is accessed through CPU0 and only 25\% through CPU1. The worst case scenario (right) is when entire sub-cube is located in memory of one CPU only }}{30}{figure.1.20}}
\newlabel{fig:Cuboid_divide}{{20}{30}{Cuboid divided into two physical memories. In ideal case (middle) half of the sub-cube is accessed through memory controller of CPU0 and the second half is accessed through memory controller of CPU1. In suboptimal case (left) 75\% of sub-cube is accessed through CPU0 and only 25\% through CPU1. The worst case scenario (right) is when entire sub-cube is located in memory of one CPU only}{figure.1.20}{}}
\newlabel{fig:Cuboid_divide@cref}{{[figure][20][]20}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Block diagram of the system partitions. GPU has six different partitions, all are used to process the queries from fact tables. CPU has one partition that processes multidimensional cube.}}{31}{figure.1.21}}
\newlabel{fig:krunal6}{{21}{31}{Block diagram of the system partitions. GPU has six different partitions, all are used to process the queries from fact tables. CPU has one partition that processes multidimensional cube}{figure.1.21}{}}
\newlabel{fig:krunal6@cref}{{[figure][21][]21}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Improving IO performance with Adaptive Compression}{31}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Adaptive compression location in data analysis for simulation analysis.}}{31}{figure.1.22}}
\newlabel{fig:krunal7}{{22}{31}{Adaptive compression location in data analysis for simulation analysis}{figure.1.22}{}}
\newlabel{fig:krunal7@cref}{{[figure][22][]22}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Two implementation of adaptive compression.}}{32}{figure.1.23}}
\newlabel{fig:krunal8}{{23}{32}{Two implementation of adaptive compression}{figure.1.23}{}}
\newlabel{fig:krunal8@cref}{{[figure][23][]23}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Data compression and transfer analysis of two implementation.}}{32}{figure.1.24}}
\newlabel{fig:krunal9}{{24}{32}{Data compression and transfer analysis of two implementation}{figure.1.24}{}}
\newlabel{fig:krunal9@cref}{{[figure][24][]24}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Adaptive Data Compression and Transfer Method.}}{33}{figure.1.25}}
\newlabel{fig:krunal10}{{25}{33}{Adaptive Data Compression and Transfer Method}{figure.1.25}{}}
\newlabel{fig:krunal10@cref}{{[figure][25][]25}{33}}
\citation{wasi2013high}
\citation{wasi2013high}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}HPC implementation of MapReduce}{34}{subsection.1.2.5}}
\citation{teijeiro2011design}
\citation{el2006upc}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces RDMA based architecture of MapReduce proposed by \cite  {wasi2013high}.}}{35}{figure.1.26}}
\newlabel{fig:RDMA_hadoop}{{26}{35}{RDMA based architecture of MapReduce proposed by \cite {wasi2013high}}{figure.1.26}{}}
\newlabel{fig:RDMA_hadoop@cref}{{[figure][26][]26}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Future Direction}{36}{section.1.3}}
\bibstyle{splncs03}
\bibdata{paper_2.bib}
\bibcite{domo}{1}
\bibcite{}{2}
\bibcite{aji2013hadoop}{3}
\bibcite{avery2011giraph}{4}
\bibcite{borthakur2011apache}{5}
\bibcite{braam2004lustre}{6}
\bibcite{chen2014big}{7}
\bibcite{chen2014diannao}{8}
\bibcite{chen2014dadiannao}{9}
\bibcite{chilimbi2014project}{10}
\bibcite{dean2012large}{11}
\bibcite{deelman2009workflows}{12}
\bibcite{demchenko2014defining}{13}
\bibcite{deng2009imagenet}{14}
\bibcite{ekanayake2010twister}{15}
\bibcite{el2006upc}{16}
\bibcite{esmaeilzadeh2012neural}{17}
\bibcite{farber1997parallel}{18}
\bibcite{fox2014towards}{19}
\bibcite{FrankSchmuck}{20}
\bibcite{gentzsch2001sun}{21}
\bibcite{ghemawat2003google}{22}
\bibcite{goodfellow2013maxout}{23}
\bibcite{grimshaw2013gffs}{24}
\bibcite{hinton2006fast}{25}
\bibcite{Jha2014}{26}
\bibcite{kashyap2015big}{27}
\bibcite{le2013building}{28}
\bibcite{low2014graphlab}{29}
\bibcite{luckow2012p}{30}
\bibcite{malewicz2010pregel}{31}
\bibcite{mantha2012pilot}{32}
\bibcite{plimpton2011mapreduce}{33}
\bibcite{Qiu2014}{34}
\bibcite{qiu2014towards}{35}
\bibcite{wasi2013high}{36}
\bibcite{raicu2008many}{37}
\bibcite{rajasekar2010irods}{38}
\bibcite{Reed2015}{39}
\bibcite{sakr2013processing}{40}
\bibcite{seo2010hama}{41}
\bibcite{shaun2007storage}{42}
\bibcite{shvachko2010hadoop}{43}
\bibcite{simard2003best}{44}
\bibcite{staples2006torque}{45}
\bibcite{teijeiro2011design}{46}
\bibcite{temam2012defect}{47}
\bibcite{vavilapalli2013apache}{48}
\bibcite{yoo2003slurm}{49}
\bibcite{zaharia2012resilient}{50}
\citation{*}
